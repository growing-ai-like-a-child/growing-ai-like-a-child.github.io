---
layout: home
---

<center>
    <h1>Growing AI like a Child, at Scale</h1>
</center>

<hr class="small" style="border-width: 1pt; border-color: lightgray;">

<div class="description" style="font-size: 13pt;">
<p>Humans never "learn" intelligence. Humans <i>develop</i> intelligence. Biological lives on this planet take heavy advantage of intelligent primitives embedded in their genes. Cats never "learn" to backflip. Birds never "learn" to fly. In the same way, humans never "learn" to <i>cognize</i>. Humans are born with a set of <i>core cognition</i>, that sets the foundation for our perception and action in the physical world. </p>
</div>

<div class="description" style="font-size: 13pt;">
<p>Our <i>core cognition</i> unravels through a specific developmental trajectory as we grow into adulthood. Here, we seek to do the same for our machines, leveraging heavy cognitive literature in developmental psychology, and in particular, Piagetian theory of cognitive development, to design our <i>growing up</i> curriculum. In addition, we also want to learn from the current success of machine intelligence, specifically the <i>scaling</i> law.</p>
</div>

<div class="description" style="font-size: 13pt;">
Instead of putting <i>growing up</i> and <i>scaling up</i> into opposite camps, we argue the next step towards human-like artificial general intelligence is to <i>grow AI like a child, at scale</i>, and call for an open-source collaborative community effort to launch the next step.
</div>

<hr class="small" style="border-width: 1pt; border-color: lightgray;">
<div class="subsubheading"><h2>Join Us: <a href="https://join.slack.com/t/growingailikeachild/shared_invite/zt-2rw1oqto3-I7IsBoawbi6btzBkaTFI8g" target="_blank">Slack</a></h2></div>
<hr class="small" style="border-width: 1pt; border-color: lightgray;">

<h3>Updates</h3>
<div class="publication" style="font-size: 12pt; font-family:'Times New Roman', Times, serif;">
    <p style="margin-bottom: 3px;">
        <a href="https://arxiv.org/abs/2410.00332" target="_blank">
            Vision Language Models Know Law of Conservation without Understanding More-or-Less
        </a>
    </p>
    <p style="margin-top: 3px;">
        TLDR: We find that Vision Language Models know the law of conservation but fail at quantitative understanding in a way opposite to human intuitive biases.
    </p>
</div>



<center>
    <figure>
        <img src="material_1.jpg" alt="Logo" style="max-width: 100%; height: auto;">
        <figcaption><a href="https://openreview.net/forum?id=fDNBPqgr4K" target="_blank">CogDevelop2K</a></figcaption>
    </figure>
</center>
